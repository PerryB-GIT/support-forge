# Module 7.3: Responsible AI Principles
## AI Launchpad Academy - HARDEN Phase
### Duration: 15 minutes

---

## VIDEO SCRIPT

[SCREEN: Contemplative visual - human and AI working together, balanced scales imagery]

We've spent this course teaching you how to build powerful AI automations. [PAUSE] Now let's talk about restraint.

[SCREEN: Title card - "Module 7.3: Responsible AI Principles"]

Just because AI can do something doesn't mean it should. And as the person building these systems, you're the one making those decisions.

This isn't about being afraid of AI. It's about being thoughtful. Let's dig in.

---

## SECTION 1: WHAT AI SHOULDN'T DO

[SCREEN: Red "no" symbol with AI icon]

Let me be direct. There are things you should never automate with AI, no matter how capable the technology becomes.

[SCREEN: List appearing one at a time]

Final decisions that significantly impact people's lives. Hiring and firing. Loan approvals. Medical diagnoses. Criminal sentencing. [PAUSE] AI can inform these decisions. AI can surface relevant information. But a human needs to make the call.

[PAUSE]

Why? Because AI doesn't understand context the way humans do. It doesn't understand that this loan applicant's credit score dropped because of a medical emergency. It doesn't understand that this job candidate's career gap was for caregiving. AI sees patterns. Humans see people.

[SCREEN: Second category appearing]

Emotional support in crisis situations. If someone reaches out in distress, expecting a human connection, an AI chatbot is not an appropriate response. This is especially true for mental health, grief, or emergency situations.

[PAUSE]

Your automation might be able to detect when someone needs help. It can route them to a human. It can provide resources. But pretending to be the support itself? That's a line you don't cross.

[SCREEN: Third category appearing]

Legal, medical, or financial advice. AI can provide general information. It can answer questions about policies. But specific advice that someone might rely on for important decisions needs a qualified human.

[PAUSE]

The common thread here: when the stakes are high and the situation is personal, human judgment is irreplaceable.

---

## SECTION 2: HUMAN OVERSIGHT - THE HUMAN IN THE LOOP

[SCREEN: Diagram showing human review checkpoint in automation flow]

Human oversight doesn't mean humans do all the work. It means humans remain in control.

The concept you'll hear is "human in the loop." This means building checkpoints into your automations where humans can review, approve, or intervene.

[SCREEN: Three levels of human oversight]

Level one: full automation with human audit. The automation runs independently. Humans review logs and outputs periodically to ensure quality.

Level two: automation with approval gates. The automation does the work but pauses before taking action. A human reviews and approves before execution.

Level three: automation as assistant. AI surfaces recommendations. Humans make the decision. AI executes based on human direction.

[PAUSE]

Which level is right? It depends on the stakes.

[SCREEN: Risk matrix - stakes vs. volume]

Low stakes, high volume? Level one. Think email categorization, data entry, routine responses.

Medium stakes or new automations? Level two. You want to watch the system until you trust it.

High stakes or customer-facing decisions? Level three. AI assists but doesn't decide.

[PAUSE]

Here's a practical implementation. In Zapier, you can add an approval step that pauses the workflow and sends a notification. Someone reviews, clicks approve or reject, and the workflow continues or stops accordingly.

[SCREEN: Zapier approval step mockup]

In n8n, you can use wait nodes with external webhooks to pause for human input.

The key: don't just build the automation. Build the oversight mechanism into it from the start.

---

## SECTION 3: UNDERSTANDING AND ADDRESSING BIAS

[SCREEN: Balance scales slightly tilted]

AI systems learn from data. And data reflects the world that created it, including its biases.

[PAUSE]

This isn't theoretical. This shows up in real systems.

[SCREEN: Examples appearing]

Resume screening AI that penalizes women's names because it learned from historically male-dominated hiring data.

Facial recognition that works less accurately on darker skin tones because training data was predominantly lighter-skinned faces.

Language models that associate certain professions with certain genders because that's what appears in the text they learned from.

[PAUSE]

As someone building automations, you need to be aware of this. Here's how:

[SCREEN: Bias mitigation strategies]

Strategy one: diverse testing. Don't just test that your automation works. Test it with diverse inputs. Different names. Different backgrounds. Different scenarios. Look for patterns in how it responds.

Strategy two: review the training data. If you're fine-tuning or training anything custom, examine your dataset. Is it representative? Is it balanced?

Strategy three: monitor outputs over time. Bias might not be obvious in one output. It might only become visible in aggregate. Periodically review patterns in your automation's decisions.

Strategy four: get external perspectives. Have someone outside your team review your system. They might catch assumptions you've internalized.

[PAUSE]

Here's the mindset shift. Don't ask "is my AI biased?" Ask "where might my AI be biased, and how would I know?" That second question leads to actual investigation.

---

## SECTION 4: TRANSPARENCY AND DISCLOSURE

[SCREEN: Speech bubble with AI label]

When someone interacts with your AI automation, do they know they're interacting with AI?

[PAUSE]

This is both an ethical question and increasingly a legal one.

The ethical case: people have a right to know when they're talking to a machine. It affects how they interpret responses, what trust they place in the interaction, and what recourse they expect.

[SCREEN: Good vs. bad disclosure examples]

Bad: an email that sounds like it's from a person but is entirely AI-generated.

Good: "This is an automated response. A team member will follow up within 24 hours."

Better: "This response was drafted by AI and reviewed by our support team."

[PAUSE]

The legal case: regulations are catching up. The EU AI Act requires disclosure when AI is used in ways that interact with people. California has disclosure requirements for bots. More jurisdictions are following.

[SCREEN: Brief mention of regulations with flags]

Even if you're not legally required to disclose, it's good practice. It builds trust. It sets appropriate expectations. And it protects you from claims of deception later.

---

## SECTION 5: DATA PRIVACY - GDPR AND CCPA BASICS

[SCREEN: Privacy shield icons with EU and US flags]

Let's talk briefly about data privacy regulations. I'm not going to make you a lawyer, but you need to know the basics.

[PAUSE]

GDPR - the General Data Protection Regulation. This is European law, but it applies to you if you handle data from anyone in the EU.

[SCREEN: GDPR key points]

Key points:

You need a legal basis to process personal data. Consent is the most common, but there are others.

People have rights over their data. Right to access it. Right to delete it. Right to know how it's used.

You must protect personal data with appropriate security measures.

If you have a data breach, you may need to report it within 72 hours.

[PAUSE]

CCPA - the California Consumer Privacy Act. Similar idea for California residents.

[SCREEN: CCPA key points]

Key points:

People can opt out of having their data sold.

People can request to see what data you have about them.

People can request deletion of their data.

You need to disclose what data you collect and why.

[PAUSE]

What does this mean for your AI automations?

[SCREEN: Practical implications for AI]

First: be careful what data you send to AI services. When you send customer data to Claude for processing, that data is going to a third party. Make sure your privacy policy covers this.

Second: honor data deletion requests. If someone asks to be deleted from your systems, that includes data in your automation logs, sheets, CRMs, everywhere.

Third: know your data flows. Document what personal data your automations touch, where it goes, and how long it's kept.

[PAUSE]

I recommend creating a simple data flow diagram for each automation that handles personal data. Where does data come from? Where does it go? Who can access it? How long is it kept?

[SCREEN: Simple data flow diagram example]

---

## SECTION 6: BUILDING RESPONSIBLY - A FRAMEWORK

[SCREEN: Framework diagram - THINK BEFORE YOU BUILD]

Let me give you a practical framework. Before you build any AI automation, run through these questions.

[SCREEN: Framework questions appearing]

Question one: what could go wrong? Spend five minutes imagining failures. Bad outputs. Misunderstandings. Edge cases. What's the worst case if this automation makes a mistake?

Question two: who is affected? Think beyond your immediate user. Who else does this automation impact? Customers? Employees? Third parties?

Question three: what oversight exists? Who reviews this automation's work? How would you catch a problem? Is there a way to intervene?

Question four: is there appropriate disclosure? Do people interacting with this know it's AI? Do they know how their data is being used?

Question five: have I tested for bias? Have I run diverse scenarios through this? Have I looked for patterns in the outputs?

[PAUSE]

This takes maybe ten minutes at the start of a project. It can save you from building something you regret.

---

## SECTION 7: WHEN THINGS GO WRONG

[SCREEN: Emergency response graphic]

Despite your best efforts, something might go wrong. AI might produce harmful output. Make a biased decision. Expose something it shouldn't. What then?

[SCREEN: Response steps]

Step one: stop the automation. Don't let it continue doing harm while you figure out what happened.

Step two: assess the damage. What happened? Who was affected? What data was involved?

Step three: notify affected parties. If customer data was exposed, or if someone received incorrect information, they need to know.

Step four: document everything. What happened, when, what you did in response. This is important for learning and potentially for legal reasons.

Step five: fix and prevent. Understand the root cause. Put controls in place so it can't happen again.

Step six: consider whether to report. Depending on the nature of the incident and your jurisdiction, you may have legal obligations to report to regulators.

[PAUSE]

Having an incident response plan before something goes wrong is much better than making it up in the moment.

---

## SECTION 8: WRAP UP

[SCREEN: Balanced visual - AI and human hands working together]

Responsible AI isn't about limiting what you can build. It's about building things that work for people, not against them.

[PAUSE]

Remember:

Some things shouldn't be automated, no matter how technically possible.

Human oversight should be built in, not bolted on.

Bias is real, and catching it requires active effort.

Transparency builds trust.

Privacy regulations apply to your automations.

Have a plan for when things go wrong.

[PAUSE]

[SCREEN: "Next: 7.4 - Documentation & Handoff"]

In our final module of the HARDEN phase, we'll talk about documentation. How do you capture what you've built so others can understand, maintain, and extend it?

[SCREEN: End card with AI Launchpad Academy logo]

See you in module seven point four.

---

## PRODUCTION NOTES

- Pacing: Thoughtful and measured. This content requires reflection.
- Visuals: Balance imagery - human and AI together, not in opposition.
- Tone: Serious but not preachy. Present this as professional responsibility, not moral lecturing.
- Examples: Use real-world examples where possible (without naming specific companies in litigation).
- Legal disclaimers: Consider adding a visual disclaimer that this is educational, not legal advice.

---

## KEY TERMS FOR PRONUNCIATION

- GDPR: "G-D-P-R" (spelled out)
- CCPA: "C-C-P-A" (spelled out)
- EU: "E-U" (spelled out)
- n8n: "en-eight-en"
- bias: "BY-us"
